---
title: "Practical Machine Learning Peer-Graded Project"
author: "Nicholas Stewart"
date: "December 27, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project preliminaries:

Author: Nicholas Stewart
Coursera course: Practical Machine Learning.
Date: December 26, 2018 15:07 (EST)

Purpose of R code:
(1) To input a training set and to train a gradient boosted model (package gbm) that learns how to classify the quality
of exercise (recorded in the classe variable).
(2) Each trained model will have its accuracy calculated.
(3) The tuning parameters n.trees, interaction.depth (varies from 1 to 4), and shrinkage are allowed to vary.
(4) The model with the highest accuracy is selected.



```{r gbm}
accuracy1<-rep(0,48)
L1<-NULL
# -----------------------------------------------------

classification_f<-function(x) { y<-rep("A",(dim(x)[1]))
  for (i in 1:(dim(x)[1])){
  m <- as.numeric(which.max(x[i,,]))
  if (m==1) {y[i]<-"A"}
  else if (m==2) {y[i]<-"B"}
  else if (m==3) {y[i]<-"C"}
  else if (m==4) {y[i]<-"D"}
  else  {y[i]<-"E"}
  
  }
return (y)
}

# ------------------------------------------------------------------------

maxFit<-NULL
library(gbm)
train<-read.csv("C:\\Users\\nstewart\\Desktop\\pml_train.csv",header=TRUE)
train$classe<-as.factor(train$classe)
train<-subset(train,select=-c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp))
e<-sample(c(1:(dim(train)[1])),0.6*(dim(train)[1]),replace=FALSE)
TRAIN<-train[e,] 
VALIDATE<-train[-e,]
test<-read.csv("C:\\Users\\nstewart\\Desktop\\pml_test.csv",header=TRUE)

test<-subset(test,select=-c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp))
shrinkage1<-c(0.1,0.01,0.001)
H<-1
acc.max<-0

# ----------------------------------------------------------------------

for (k1 in 1:1) {   # consider number of trees = 1000 for efficiency of markdown code
  for (s in 1:3)    {
    for (d in 1:1)      {
    k=k1*1000
    
    set.seed(101)
    
    fit<-gbm(classe ~ ., data = TRAIN, distribution = "multinomial", n.trees = k, shrinkage = shrinkage1[s], interaction.depth=d)
    pred<-predict(fit,newdata = VALIDATE,n.trees = fit$n.trees,type="response")
    pred_class<-classification_f(pred)
    accuracy1[H]<-sum(VALIDATE$classe==pred_class)/length(pred_class)
    L<-c(accuracy1[H],k,shrinkage1[s],d)
    
    L1<-rbind(L1,L) # grid with values of tuning parameters and accuracy statistic...
    
    if (accuracy1[H] > acc.max) {acc.max<-accuracy1[H]
                                 maxFit<-fit}
    
    print(" ")
    print(H)
    print(" ")
    
    H<-H+1
    
    } # end for d in 1:4 for depth
  } # end for s in 1:3 (shrinkage)
} # end for k1 in 1:10 (calculate number of trees)

colnames(L1)<-c("Accuracy","Number_of_trees","Shrinkage","Depth")
L1<-data.frame(L1)   # create data frame...
print(L1)
pred<-predict(maxFit,newdata = test, n.trees= maxFit$n.trees, type = "response")  
pred_class2<-classification_f(pred)   
print(pred_class2)   
```

# PRINT OUT immediately above is the classification as judged by the "best" model.




